{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b4b2c2",
   "metadata": {},
   "source": [
    "# Web scraper\n",
    "This is the function i used to scrape figma pages for information, Which i then processed using an llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c76cc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.figma.com/resource-library/graphic-design-principles/\"\n",
    "response = requests.get(url)\n",
    "print(f\"Status code: {response.status_code}\")\n",
    "soup = BeautifulSoup(response.text, \"html\")\n",
    "\n",
    "import re\n",
    "def remove_html_tags(text):\n",
    "  pattern = re.compile('<.*?>')\n",
    "  return pattern.sub(r'', text)\n",
    "cleaned_text = remove_html_tags(soup.get_text())\n",
    "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "  file.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871ecca",
   "metadata": {},
   "source": [
    "# Data formatting function\n",
    "In order to turn data from a dataset into a form that the llama chat model would understand, extensive replacement of human and assistant keys with [INST] tags was required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cafc26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_qa_to_llama_format(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    converted_data = []\n",
    "    for item in data:\n",
    "        formatted_text = f\"<s>[INST] {item['human']} [/INST] {item['assistant']}</s>\"\n",
    "        converted_data.append({\"text\": formatted_text})\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "convert_qa_to_llama_format('content.json', 'llama.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c74511",
   "metadata": {},
   "source": [
    "# Web dev dataset\n",
    "This is the dataset we used to train the model on code and web dev principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "import os\n",
    "\n",
    "dataset = load_dataset(\"sahil2801/CodeAlpaca-20k\")\n",
    "\n",
    "dataset['train'].to_json('CodeAlpaca-20k.json')\n",
    "\n",
    "train_data = dataset['train']\n",
    "print(f\"Number of examples: {len(train_data)}\")\n",
    "print(train_data[0])  # First example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2649a2f",
   "metadata": {},
   "source": [
    "# Reformatting\n",
    "Here I applyied the earlier function to the downloaded dataset to get fully formatted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d990bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 20022 examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def convert_codealpaca_to_llama(input_file, output_file):\n",
    "    converted_data = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                item = json.loads(line)\n",
    "                \n",
    "                instruction = item['instruction'].strip()\n",
    "                input_text = item.get('input', '').strip()\n",
    "                output_text = item['output'].strip()\n",
    "                if input_text and input_text not in ['', '< noinput >', '<noinput>']:\n",
    "                    user_prompt = f\"{instruction}\\n\\n{input_text}\"\n",
    "                else:\n",
    "                    user_prompt = instruction\n",
    "                formatted_text = f\"<s>[INST] {user_prompt} [/INST] {output_text}</s>\"\n",
    "                converted_data.append({\"text\": formatted_text})\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"Converted {len(converted_data)} examples\")\n",
    "\n",
    "convert_codealpaca_to_llama('CodeAlpaca-20k.json', 'llama_codealpaca.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1f8c6",
   "metadata": {},
   "source": [
    "# Supplementary dataset\n",
    "Data from this was not directly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f90cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Building message lookup...\n",
      "Converting conversations...\n",
      "Processed 1000 conversations...\n",
      "Processed 2000 conversations...\n",
      "Processed 3000 conversations...\n",
      "Processed 4000 conversations...\n",
      "Processed 5000 conversations...\n",
      "Processed 6000 conversations...\n",
      "Processed 7000 conversations...\n",
      "Processed 8000 conversations...\n",
      "Processed 9000 conversations...\n",
      "Processed 10000 conversations...\n",
      "Saving 10000 conversations...\n",
      "Done! Saved 10000 conversations to llama_oasst2.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "def convert_oasst2_to_llama_fast(output_file=\"llama_oasst2.json\", max_examples=10000):\n",
    "   print(\"Loading dataset...\")\n",
    "   dataset = load_dataset(\"OpenAssistant/oasst2\")\n",
    "   \n",
    "   converted_data = []\n",
    "   count = 0\n",
    "   print(\"Building message lookup...\")\n",
    "   msg_lookup = {item['message_id']: item for item in dataset['train']}\n",
    "   \n",
    "   print(\"Converting conversations...\")\n",
    "   for item in dataset['train']:\n",
    "       if count >= max_examples:\n",
    "           break\n",
    "       if (item['role'] == 'assistant' and \n",
    "           item['parent_id'] and \n",
    "           item.get('review_result', False) == True):\n",
    "           \n",
    "           parent = msg_lookup.get(item['parent_id'])\n",
    "           \n",
    "           if (parent and \n",
    "               parent['role'] == 'prompter' and \n",
    "               item['lang'] == 'en'): \n",
    "               \n",
    "               user_text = parent['text'].strip()\n",
    "               assistant_text = item['text'].strip()\n",
    "               if 10 < len(user_text) < 2000 and 10 < len(assistant_text) < 4000:\n",
    "                   conversation = f\"<s>[INST] {user_text} [/INST] {assistant_text}</s>\"\n",
    "                   converted_data.append({\"text\": conversation})\n",
    "                   count += 1\n",
    "                   \n",
    "                   if count % 1000 == 0:\n",
    "                       print(f\"Processed {count} conversations...\")\n",
    "   \n",
    "   print(f\"Saving {len(converted_data)} conversations...\")\n",
    "   with open(output_file, 'w', encoding='utf-8') as f:\n",
    "       json.dump(converted_data, f, indent=2, ensure_ascii=False)\n",
    "   \n",
    "   print(f\"Done! Saved {len(converted_data)} conversations to {output_file}\")\n",
    "   return len(converted_data)\n",
    "count = convert_oasst2_to_llama_fast()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41901b99",
   "metadata": {},
   "source": [
    "# Langchain dataset\n",
    "The dataset used to train the model on langchain principles, downloaded and formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hudsongeorge/langchain-docs\")\n",
    "\n",
    "howto_dataset = load_dataset(\"LangChainDatasets/langchain-howto-queries\")\n",
    "\n",
    "def convert_langchain_docs_to_llama(dataset):\n",
    "    converted = []\n",
    "    for item in dataset['train']:\n",
    "        if 'question' in item and 'answer' in item:\n",
    "            text = f\"<s>[INST] {item['question']} [/INST] {item['answer']}</s>\"\n",
    "            converted.append({\"text\": text})\n",
    "    with open(\"lang.json\", 'w', encoding='utf-8') as f:\n",
    "       json.dump(converted, f, indent=2, ensure_ascii=False)\n",
    "converted = convert_langchain_docs_to_llama(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6016b",
   "metadata": {},
   "source": [
    "# AI principles of usage dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ceea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Pavithrars/AI_dataset\")\n",
    "def to_llama_format(example):\n",
    "    instruction = example.get(\"Question\") or example.get(\"instruction\") or \"Explain AI.\"\n",
    "    output = example.get(\"Answer\") or example.get(\"output\") or \"Artificial Intelligence is...\"\n",
    "    text = f\"<s>[INST] <<SYS>>\\nYou are a helpful assistant that explains how AI works.\\n<</SYS>>\\n\\n{instruction} [/INST] {output} </s>\"\n",
    "    return {\"text\": text}\n",
    "train_dataset = dataset[\"train\"].map(to_llama_format)\n",
    "data_list = [row for row in train_dataset]\n",
    "with open(\"llama_ai_dataset.json\", \"w\") as f:\n",
    "    json.dump(data_list, f, indent=2)\n",
    "\n",
    "print(\"Saved llama_ai_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9346d",
   "metadata": {},
   "source": [
    "# Dataset Reformatting code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0249e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset size: 173 examples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"llama_ai_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "cleaned = [{\"text\": item[\"text\"]} for item in data if \"text\" in item]\n",
    "\n",
    "with open(\"llama_ai_dataset_clean.json\", \"w\") as f:\n",
    "    json.dump(cleaned, f, indent=2)\n",
    "\n",
    "print(f\"Cleaned dataset size: {len(cleaned)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393df78d",
   "metadata": {},
   "source": [
    "# Additional AI Info dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"lucadiliello/How_AI_Works\")\n",
    "\n",
    "records = []\n",
    "\n",
    "for item in dataset[\"train\"]:\n",
    "    question = item.get(\"title\") or item.get(\"question\") or \"Explain this AI concept.\"\n",
    "    answer = item.get(\"document\") or item.get(\"content\") or item.get(\"summary\") or \"No answer available.\"\n",
    "    text = f\"<s>[INST] <<SYS>>\\nYou are a helpful assistant that explains how AI works.\\n<</SYS>>\\n\\n{question} [/INST] {answer} </s>\"\n",
    "\n",
    "    records.append({\"text\": text})\n",
    "with open(\"llama_ai_dataset2.json\", \"w\") as f:\n",
    "    json.dump(records, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Saved {len(records)} examples in llama_ai_dataset.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e71675",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "# After Data consolidation into a final file, Fine tuning started from these notebooks, on google collab.\n",
    "The code below is to load the dataset we used for tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3481bc",
   "metadata": {},
   "source": [
    "# Necessary Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U accelerate peft bitsandbytes transformers trl datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba617473",
   "metadata": {},
   "source": [
    "# Huggingface login to access gated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedda676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcfbce3",
   "metadata": {},
   "source": [
    "# Actual codebase used to fine tune model\n",
    "QLoRA Was used to fine tune the loaded llama 7b chat model. LoRA config and trainer Config is below, Marked with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d91e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Loading Model in 4-bit (QLoRA)\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype=\"float16\"\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Loading Dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"final.json\", split=\"train\")\n",
    "train_ds, val_ds = dataset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "\n",
    "# LoRA Config\n",
    "peft_config = LoraConfig(\n",
    "    r=32,                \n",
    "    lora_alpha=16,        \n",
    "    lora_dropout=0.05,    \n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qlora-llama7b-finetuned\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,     \n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=500,  \n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,                          \n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=2           \n",
    ")\n",
    "\n",
    "\n",
    "# Trainer Initialisation\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_ds,      \n",
    "    eval_dataset=val_ds,         \n",
    "    peft_config=peft_config,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "\n",
    "# Actual Training\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "\n",
    "# Saving Adapter post training\n",
    "# The adapter is saved, not the full model. The adapter is the additional weights learned during fine-tuning.\n",
    "\n",
    "model.save_pretrained(\"./qlora-llama7b-adapter\")\n",
    "tokenizer.save_pretrained(\"./qlora-llama7b-adapter\")\n",
    "\n",
    "print(\"Model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5f418",
   "metadata": {},
   "source": [
    "# Final Rag content production\n",
    "Here, most of the data used in fune tuning was parsed into separate files for the rag pipeline's vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_json_to_continuous_text(json_file_path, output_file_path=None):\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "        \n",
    "        parsed_conversations = []\n",
    "        \n",
    "        for conversation in data:\n",
    "            human_text = conversation.get('text', '').strip()\n",
    "            \n",
    "            continuous_text = f\"{human_text}\".strip()\n",
    "            parsed_conversations.append(continuous_text)\n",
    "\n",
    "        if output_file_path:\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                for text in parsed_conversations:\n",
    "                    output_file.write(text + '\\n\\n')\n",
    "            print(f\"Parsed text saved to: {output_file_path}\")\n",
    "        \n",
    "        return parsed_conversations\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{json_file_path}' not found.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in '{json_file_path}'.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing file: {str(e)}\")\n",
    "        return []\n",
    "    \n",
    "parse_json_to_continuous_text(\"final.json\", \"web_cont.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62542d26",
   "metadata": {},
   "source": [
    "# Data conersions\n",
    "Data was further converted into QA pairs for data extraction for part of the data used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731bee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def convert_to_qa_format(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    converted_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        text = item.get('text', '')\n",
    "        pattern = r'<s>\\[INST\\]\\s*(.*?)\\s*\\[/INST\\]\\s*(.*?)\\s*</s>'\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            instruction = match.group(1).strip()\n",
    "            response = match.group(2).strip()\n",
    "            \n",
    "            qa_item = {\n",
    "                \"human\": instruction,\n",
    "                \"assistant\": response\n",
    "            }\n",
    "            converted_data.append(qa_item)\n",
    "        else:\n",
    "            print(f\"Warning: Could not parse item: {text[:50]}...\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(converted_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Conversion complete! Converted {len(converted_data)} items.\")\n",
    "    return converted_data\n",
    "\n",
    "def convert_data_to_qa_format(data):\n",
    "    converted_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        text = item.get('text', '')\n",
    "        pattern = r'<s>\\[INST\\]\\s*(.*?)\\s*\\[/INST\\]\\s*(.*?)\\s*</s>'\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            instruction = match.group(1).strip()\n",
    "            response = match.group(2).strip()\n",
    "            \n",
    "            qa_item = {\n",
    "                \"human\": instruction,\n",
    "                \"assistant\": response\n",
    "            }\n",
    "            converted_data.append(qa_item)\n",
    "    \n",
    "    return converted_data\n",
    "\n",
    "def convert_to_plain_text(data_or_file, output_file=None):\n",
    "    if isinstance(data_or_file, str):\n",
    "        with open(data_or_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = data_or_file\n",
    "    \n",
    "    output_lines = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        text = item.get('text', '')\n",
    "        pattern = r'<s>\\[INST\\]\\s*(.*?)\\s*\\[/INST\\]\\s*(.*?)\\s*</s>'\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        \n",
    "        if match:\n",
    "            instruction = match.group(1).strip()\n",
    "            response = match.group(2).strip()\n",
    "            output_lines.append(f\"human: {instruction}\")\n",
    "            output_lines.append(f\"assistant: {response}\")\n",
    "            if i < len(data) - 1:\n",
    "                output_lines.append(\"\")\n",
    "    \n",
    "    result_text = \"\\n\".join(output_lines)\n",
    "    if output_file:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(result_text)\n",
    "        print(f\"Plain text format saved to {output_file}\")\n",
    "    \n",
    "    return result_text\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  convert_to_plain_text(\"final.json\", \"web_mid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b28a9b",
   "metadata": {},
   "source": [
    "# Dummy run of fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "model = PeftModel.from_pretrained(base_model, \"your-username/your-model-name\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"your-username/your-model-name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
